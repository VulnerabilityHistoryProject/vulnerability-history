require_relative 'base_loader'

class GitLoader extend BaseLoader
  def load_data(repo_path)
    File.open("#{repo_path}/commits/gitlog.txt") do |f|
      get_commits(f)
    end
  end

  private

  def get_commits(file)
    commit_queue= Array.new
    isCommitSec = false
    isStarted = false

    file.each_line do |line|
      if line.strip =~ (/^:::$/) or line.strip =~ (/^;;;:::$/)
        #begin processing, encountered
        #first commit (:::) avoid adding
        #any garbage before first commit
        if not isStarted
          isStarted = true
        else
          # Process commits, we already have queued the commit
          commit, hash = regex_process_commit(commit_queue)
          index_process_commit(commit, hash)
        end

      else
        #avoid adding empty string or havent started yet
        commit_queue.push(line) unless line.strip == "" or not isStarted
      end
    end#loop

    #process commits
    commit, hash = regex_process_commit(commit_queue)
    index_process_commit(commit, hash)
  end

  def regex_process_commit(arr)
    message = ""
    filepaths = {}
    end_message_index = 0
    hash = Hash.new
    in_files = false # Have we gotten to the file portion yet? After the ;;; delimiter

    #index 5 should be the start
    #of the message
    (5..arr.size-1).each do |i|
      #add lines to message string if they are not identifers of lines we want to get later
      if not arr.fetch(i) =~ (/^;;;/)

        #concat the message into 1
        #string
        message = message + " " + arr.fetch(i)
      else
        #get the last index of the
        #message, is it multiple
        #lines
        end_message_index = i
        break
      end
    end

    hash[:message] = message
    arr[5] = message

    #remove the multi line message since we condensed it
    (6..end_message_index).each do |i|
      arr.delete(i)
    end

    arr.each do |element|
      if fast_match(element, /^;;;/)
        in_files = true

      elsif in_files and element.include?('|')  # stats output needs to have a pipe
        # collect the filepath and churn count, store in the filepath hash
        split = element.split('|')
        filepaths[split[0].strip] = split[1].to_i

      end#if

    end#arr.each
    hash[:filepaths] = filepaths

    return arr, hash

  end#regex_process_commit

  def fast_match(str, pattern)
    return not((str =~ pattern).nil?)
  end

  def index_process_commit(arr, hash)
    commit_hash_str = arr[0].strip
    author_email_str = arr[1].strip
    created_at_tstamp = DateTime.parse(arr[3].strip)
    parent_hash = arr[4].strip

    #add commit hash
    hash[:commit_hash] = commit_hash_str[0..254]
    puts "WARNING! Hash too long #{commit_hash_str}" if commit_hash_str.length > 254

    #add email
    hash[:author_email] = author_email_str[0..254]
    puts "WARNING! Email too long #{author_email_str}" if author_email_str.length > 254

    #add Date/Time created
    hash[:created_at] = created_at_tstamp

    add_to_db(hash)

    arr.clear

  end#process_commit

  def add_to_db(hash)
      dev = Developer.new
      dev.email = hash[:author_email]
      dev.save

      commit = Commit.new
      commit.commit_hash = hash[:commit_hash]
      commit.author_id = Developer.find_by_email(hash[:author_email]).id
      commit.message = hash[:message]
      commit.date_created = hash[:created_at]
      commit.notes = ""
      commit.save

    hash[:filepaths].each do |path, churn|
      fp = Filepath.new
      fp.filepath = path
      fp.notes = ""
      fp.save

      commit_filepath = CommitFilepath.new
      commit_filepath.filepath_id = (Filepath.find_by filepath: path).id
      commit_filepath.commit_id = (Commit.find_by commit_hash: hash[:commit_hash]).id
      commit_filepath.total_churn = churn
      commit_filepath.save
      Event.create(detail: commit_filepath)
    end
  end
end
